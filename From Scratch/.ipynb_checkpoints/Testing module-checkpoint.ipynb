{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python Main_script.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_record(text,master_list):\n",
    "    text_dict = {i: 1 if i in text else 0  for i in master_list}\n",
    "        \n",
    "    return text_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yellowbrick\n",
      "  Downloading yellowbrick-1.4-py3-none-any.whl (274 kB)\n",
      "Requirement already satisfied: cycler>=0.10.0 in c:\\users\\ashup\\.conda\\envs\\new_cryptic\\lib\\site-packages (from yellowbrick) (0.11.0)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.2 in c:\\users\\ashup\\.conda\\envs\\new_cryptic\\lib\\site-packages (from yellowbrick) (3.5.1)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\ashup\\.conda\\envs\\new_cryptic\\lib\\site-packages (from yellowbrick) (1.8.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\users\\ashup\\.conda\\envs\\new_cryptic\\lib\\site-packages (from yellowbrick) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\ashup\\.conda\\envs\\new_cryptic\\lib\\site-packages (from yellowbrick) (1.22.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\ashup\\.conda\\envs\\new_cryptic\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.4.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ashup\\.conda\\envs\\new_cryptic\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (4.32.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ashup\\.conda\\envs\\new_cryptic\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ashup\\.conda\\envs\\new_cryptic\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\ashup\\.conda\\envs\\new_cryptic\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (3.0.8)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\ashup\\.conda\\envs\\new_cryptic\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (9.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ashup\\.conda\\envs\\new_cryptic\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.16.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\ashup\\.conda\\envs\\new_cryptic\\lib\\site-packages (from scikit-learn>=1.0.0->yellowbrick) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ashup\\.conda\\envs\\new_cryptic\\lib\\site-packages (from scikit-learn>=1.0.0->yellowbrick) (3.1.0)\n",
      "Installing collected packages: yellowbrick\n",
      "Successfully installed yellowbrick-1.4\n"
     ]
    }
   ],
   "source": [
    "!pip install yellowbrick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dw configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X, y, model, visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Configuration file not found at C:\\Users\\ashup/.dw/config.To fix this issue, run dw configure",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [48]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatadotworld\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdw\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mdw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjonloyens/intermediate-data-world\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauto_update\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\new_cryptic\\lib\\site-packages\\datadotworld\\__init__.py:98\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(dataset_key, force_update, auto_update, profile, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_dataset\u001b[39m(dataset_key, force_update\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, auto_update\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     66\u001b[0m                  profile\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;124;03m\"\"\"Load a dataset from the local filesystem, downloading it from data.world\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m    first, if necessary.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03m    ['changelog', 'datadotworldbballstats', 'datadotworldbballteam']\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _get_instance(profile, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39m \\\n\u001b[0;32m     99\u001b[0m         load_dataset(dataset_key,\n\u001b[0;32m    100\u001b[0m                      force_update\u001b[38;5;241m=\u001b[39mforce_update,\n\u001b[0;32m    101\u001b[0m                      auto_update\u001b[38;5;241m=\u001b[39mauto_update)\n",
      "File \u001b[1;32m~\\.conda\\envs\\new_cryptic\\lib\\site-packages\\datadotworld\\__init__.py:60\u001b[0m, in \u001b[0;36m_get_instance\u001b[1;34m(profile, **kwargs)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     config_param \u001b[38;5;241m=\u001b[39m (ChainedConfig(config_chain\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     55\u001b[0m         InlineConfig(kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauth_token\u001b[39m\u001b[38;5;124m'\u001b[39m)),\n\u001b[0;32m     56\u001b[0m         EnvConfig(),\n\u001b[0;32m     57\u001b[0m         FileConfig()])\n\u001b[0;32m     58\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m profile \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     59\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m FileConfig(profile\u001b[38;5;241m=\u001b[39mprofile))\n\u001b[1;32m---> 60\u001b[0m     instance \u001b[38;5;241m=\u001b[39m \u001b[43mDataDotWorld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_param\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     __instances[profile] \u001b[38;5;241m=\u001b[39m instance\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m instance\n",
      "File \u001b[1;32m~\\.conda\\envs\\new_cryptic\\lib\\site-packages\\datadotworld\\datadotworld.py:57\u001b[0m, in \u001b[0;36mDataDotWorld.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config \u001b[38;5;241m=\u001b[39m config \u001b[38;5;129;01mor\u001b[39;00m ChainedConfig()\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_client \u001b[38;5;241m=\u001b[39m \u001b[43mRestApiClient\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\new_cryptic\\lib\\site-packages\\datadotworld\\client\\api.py:56\u001b[0m, in \u001b[0;36mRestApiClient.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_host \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/v0\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(API_HOST)\n\u001b[0;32m     53\u001b[0m swagger_client \u001b[38;5;241m=\u001b[39m _swagger\u001b[38;5;241m.\u001b[39mApiClient(\n\u001b[0;32m     54\u001b[0m     host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_host,\n\u001b[0;32m     55\u001b[0m     header_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAuthorization\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m---> 56\u001b[0m     header_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBearer \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauth_token\u001b[49m))\n\u001b[0;32m     57\u001b[0m swagger_client\u001b[38;5;241m.\u001b[39muser_agent \u001b[38;5;241m=\u001b[39m _user_agent()\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_api_client \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[0;32m     60\u001b[0m     ContentNegotiatingApiClient,\n\u001b[0;32m     61\u001b[0m     host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_host,\n\u001b[0;32m     62\u001b[0m     header_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAuthorization\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     63\u001b[0m     header_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBearer \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mauth_token),\n\u001b[0;32m     64\u001b[0m     user_agent\u001b[38;5;241m=\u001b[39m_user_agent())\n",
      "File \u001b[1;32m~\\.conda\\envs\\new_cryptic\\lib\\site-packages\\datadotworld\\config.py:212\u001b[0m, in \u001b[0;36mChainedConfig.__getattribute__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, item):\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;124;03m\"\"\"Delegates requests to config objects in the chain\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_first_not_none\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_config_chain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\new_cryptic\\lib\\site-packages\\datadotworld\\config.py:227\u001b[0m, in \u001b[0;36mChainedConfig._first_not_none\u001b[1;34m(seq, supplier_func)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;124;03m\"\"\"Applies supplier_func to each element in seq, returns 1st not None\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \n\u001b[0;32m    220\u001b[0m \u001b[38;5;124;03m:param seq: Sequence of object\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03m:type supplier_func: function\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m seq:\n\u001b[1;32m--> 227\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43msupplier_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[1;32m~\\.conda\\envs\\new_cryptic\\lib\\site-packages\\datadotworld\\config.py:214\u001b[0m, in \u001b[0;36mChainedConfig.__getattribute__.<locals>.<lambda>\u001b[1;34m(c)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, item):\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;124;03m\"\"\"Delegates requests to config objects in the chain\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_first_not_none\u001b[39m\u001b[38;5;124m'\u001b[39m)(\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_config_chain\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m--> 214\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m c: \u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\new_cryptic\\lib\\site-packages\\datadotworld\\config.py:125\u001b[0m, in \u001b[0;36mFileConfig.auth_token\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mauth_token\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__validate_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config_parser\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_section, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauth_token\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\new_cryptic\\lib\\site-packages\\datadotworld\\config.py:147\u001b[0m, in \u001b[0;36mFileConfig.__validate_config\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__validate_config\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39misfile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config_file_path):\n\u001b[1;32m--> 147\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    148\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConfiguration file not found at \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    149\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTo fix this issue, run dw configure\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    150\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config_file_path))\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config_parser\u001b[38;5;241m.\u001b[39mhas_option(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_section, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauth_token\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    152\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    153\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m profile is not properly configured. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    154\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTo fix this issue, run dw -p \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m configure\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    155\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Configuration file not found at C:\\Users\\ashup/.dw/config.To fix this issue, run dw configure"
     ]
    }
   ],
   "source": [
    "import datadotworld as dw\n",
    "ds = dw.load_dataset('jonloyens/intermediate-data-world', auto_update=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kmodes\n",
      "  Downloading kmodes-0.12.1-py2.py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.22.0 in c:\\users\\ashup\\.conda\\envs\\new_cryptic\\lib\\site-packages (from kmodes) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\ashup\\.conda\\envs\\new_cryptic\\lib\\site-packages (from kmodes) (1.22.3)\n",
      "Requirement already satisfied: scipy>=0.13.3 in c:\\users\\ashup\\.conda\\envs\\new_cryptic\\lib\\site-packages (from kmodes) (1.8.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\ashup\\.conda\\envs\\new_cryptic\\lib\\site-packages (from kmodes) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ashup\\.conda\\envs\\new_cryptic\\lib\\site-packages (from scikit-learn>=0.22.0->kmodes) (3.1.0)\n",
      "Installing collected packages: kmodes\n",
      "Successfully installed kmodes-0.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install kmodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datadotworld\n",
      "  Downloading datadotworld-1.8.2-py2.py3-none-any.whl (423 kB)\n",
      "Collecting configparser<4.0a,>=3.5.0\n",
      "  Downloading configparser-3.8.1-py2.py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: click<9.0a,>=8.0 in c:\\users\\ashup\\.conda\\envs\\new_cryptic\\lib\\site-packages (from datadotworld) (8.1.2)\n",
      "Requirement already satisfied: certifi>=2017.04.17 in c:\\users\\ashup\\.conda\\envs\\new_cryptic\\lib\\site-packages (from datadotworld) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<2.0a,>=1.15 in c:\\users\\ashup\\.conda\\envs\\new_cryptic\\lib\\site-packages (from datadotworld) (1.26.9)\n",
      "Requirement already satisfied: requests<3.0a,>=2.22.0 in c:\\users\\ashup\\.conda\\envs\\new_cryptic\\lib\\site-packages (from datadotworld) (2.27.1)\n",
      "Requirement already satisfied: python-dateutil<3.0a,>=2.6.0 in c:\\users\\ashup\\.conda\\envs\\new_cryptic\\lib\\site-packages (from datadotworld) (2.8.2)\n",
      "Collecting tableschema<2.0a,>=1.5.2\n",
      "  Downloading tableschema-1.20.2-py2.py3-none-any.whl (68 kB)\n",
      "Requirement already satisfied: six<2.0a,>=1.5.0 in c:\\users\\ashup\\.conda\\envs\\new_cryptic\\lib\\site-packages (from datadotworld) (1.16.0)\n",
      "Collecting datapackage<2.0a,>=1.6.2\n",
      "  Downloading datapackage-1.15.2-py2.py3-none-any.whl (85 kB)\n",
      "Collecting tabulator>=1.22.0\n",
      "  Downloading tabulator-1.53.5-py2.py3-none-any.whl (72 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\ashup\\.conda\\envs\\new_cryptic\\lib\\site-packages (from click<9.0a,>=8.0->datadotworld) (0.4.4)\n",
      "Collecting jsonpointer>=1.10\n",
      "  Downloading jsonpointer-2.3-py2.py3-none-any.whl (7.8 kB)\n",
      "Requirement already satisfied: jsonschema>=2.5 in c:\\users\\ashup\\.conda\\envs\\new_cryptic\\lib\\site-packages (from datapackage<2.0a,>=1.6.2->datadotworld) (4.4.0)\n",
      "Collecting unicodecsv>=0.14\n",
      "  Downloading unicodecsv-0.14.1.tar.gz (10 kB)\n",
      "Collecting chardet>=3.0\n",
      "  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\ashup\\.conda\\envs\\new_cryptic\\lib\\site-packages (from jsonschema>=2.5->datapackage<2.0a,>=1.6.2->datadotworld) (21.4.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\ashup\\.conda\\envs\\new_cryptic\\lib\\site-packages (from jsonschema>=2.5->datapackage<2.0a,>=1.6.2->datadotworld) (0.18.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ashup\\.conda\\envs\\new_cryptic\\lib\\site-packages (from requests<3.0a,>=2.22.0->datadotworld) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\ashup\\.conda\\envs\\new_cryptic\\lib\\site-packages (from requests<3.0a,>=2.22.0->datadotworld) (2.0.12)\n",
      "Collecting cached-property>=1.5\n",
      "  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting isodate>=0.5.4\n",
      "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "Collecting rfc3986>=1.1.0\n",
      "  Downloading rfc3986-2.0.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting openpyxl>=2.6\n",
      "  Downloading openpyxl-3.0.9-py2.py3-none-any.whl (242 kB)\n",
      "Collecting jsonlines>=1.1\n",
      "  Downloading jsonlines-3.0.0-py3-none-any.whl (8.5 kB)\n",
      "Collecting xlrd>=1.0\n",
      "  Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n",
      "Collecting sqlalchemy>=0.9.6\n",
      "  Downloading SQLAlchemy-1.4.35-cp39-cp39-win_amd64.whl (1.6 MB)\n",
      "Collecting ijson>=3.0.3\n",
      "  Downloading ijson-3.1.4-cp39-cp39-win_amd64.whl (48 kB)\n",
      "Collecting linear-tsv>=1.0\n",
      "  Downloading linear-tsv-1.1.0.tar.gz (9.6 kB)\n",
      "Collecting boto3>=1.9\n",
      "  Downloading boto3-1.21.46-py3-none-any.whl (132 kB)\n",
      "Collecting s3transfer<0.6.0,>=0.5.0\n",
      "  Using cached s3transfer-0.5.2-py3-none-any.whl (79 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1\n",
      "  Using cached jmespath-1.0.0-py3-none-any.whl (23 kB)\n",
      "Collecting botocore<1.25.0,>=1.24.46\n",
      "  Downloading botocore-1.24.46-py3-none-any.whl (8.7 MB)\n",
      "Collecting et-xmlfile\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Collecting greenlet!=0.4.17\n",
      "  Downloading greenlet-1.1.2-cp39-cp39-win_amd64.whl (101 kB)\n",
      "Building wheels for collected packages: linear-tsv, unicodecsv\n",
      "  Building wheel for linear-tsv (setup.py): started\n",
      "  Building wheel for linear-tsv (setup.py): finished with status 'done'\n",
      "  Created wheel for linear-tsv: filename=linear_tsv-1.1.0-py3-none-any.whl size=7400 sha256=3dc7b2b5ef1d9ade15febed226212fdc5d89ad40622b9b03e3532410f3e509bf\n",
      "  Stored in directory: c:\\users\\ashup\\appdata\\local\\pip\\cache\\wheels\\b5\\eb\\b6\\e409f80d7fec532e4240dda9562cad374257d2dd010b40cfff\n",
      "  Building wheel for unicodecsv (setup.py): started\n",
      "  Building wheel for unicodecsv (setup.py): finished with status 'done'\n",
      "  Created wheel for unicodecsv: filename=unicodecsv-0.14.1-py3-none-any.whl size=10768 sha256=fda0b8755427d829e7c28b3b46a3d0bb7c3a8910233885e2ea8da1354799e385\n",
      "  Stored in directory: c:\\users\\ashup\\appdata\\local\\pip\\cache\\wheels\\d8\\c8\\27\\b237d3378d5c9ed25c2c63d9af1b3d5ccb99934f3dd030de87\n",
      "Successfully built linear-tsv unicodecsv\n",
      "Installing collected packages: jmespath, botocore, s3transfer, greenlet, et-xmlfile, xlrd, unicodecsv, sqlalchemy, openpyxl, linear-tsv, jsonlines, ijson, chardet, boto3, tabulator, rfc3986, isodate, cached-property, tableschema, jsonpointer, datapackage, configparser, datadotworld\n",
      "Successfully installed boto3-1.21.46 botocore-1.24.46 cached-property-1.5.2 chardet-4.0.0 configparser-3.8.1 datadotworld-1.8.2 datapackage-1.15.2 et-xmlfile-1.1.0 greenlet-1.1.2 ijson-3.1.4 isodate-0.6.1 jmespath-1.0.0 jsonlines-3.0.0 jsonpointer-2.3 linear-tsv-1.1.0 openpyxl-3.0.9 rfc3986-2.0.0 s3transfer-0.5.2 sqlalchemy-1.4.35 tableschema-1.20.2 tabulator-1.53.5 unicodecsv-0.14.1 xlrd-2.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install datadotworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kmodes.kmodes import KModes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import datetime\n",
    "import nltk\n",
    "from nltk import Text\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.tokenize import sent_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "#sns.set()\n",
    "from kmodes.kmodes import KModes\n",
    "import datadotworld as dw\n",
    "\n",
    "\n",
    "def dw_get(dataset_name, table_name):\n",
    "    results = dw.query(dataset_name, \"SELECT * FROM `{}`\".format(table_name))\n",
    "    df = results.dataframe\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_record(text,master_list):\n",
    "    text_dict = {i: 1 if i in text else 0  for i in master_list}\n",
    "        \n",
    "    return text_dict\n",
    "\n",
    "def get_cleaned_dw_data():\n",
    "    \n",
    "\n",
    "    coin_info = dw_get('cnoza/cryptocurrencies', 'eur')\n",
    "\n",
    "    df=coin_info[['symbol','tags']]\n",
    "    df['clean_tags']=df['tags'].str.replace('\"', '').str.replace('[', '').str.replace(']', '')\n",
    "    df['clean_tags']=df['clean_tags'].apply(lambda x: x.split(\",\"))\n",
    "    df['clean_tags_str']  = df['clean_tags'].apply(lambda x:\" \".join(x))\n",
    "\n",
    "    df1=df\n",
    "    master_list = np.unique((\",\".join(df1['clean_tags'].apply(lambda x: ','.join(map(str, x))).to_list())).split(\",\"))\n",
    "\n",
    "    \n",
    "    #creating records\n",
    "    count_vectors = pd.DataFrame.from_records(df1.clean_tags.apply(lambda x: create_record(x,master_list)).to_list())\n",
    "    count_vectors=count_vectors[[i for i in (count_vectors.columns) if \"portfolio\" not in i]]\n",
    "    return count_vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eyJhbGciOiJIUzUxMiJ9.eyJzdWIiOiJweXRob246YXNoaXNocGFuY2hhbCIsImlzcyI6ImNsaWVudDpweXRob246YWdlbnQ6YXNoaXNocGFuY2hhbDo6ODlhNTc5NDItNWRmYi00NTgwLWEwMzctZjM0NmU3MTQxMzc3IiwiaWF0IjoxNjUwNjk0MTI4LCJyb2xlIjpbInVzZXJfYXBpX2FkbWluIiwidXNlcl9hcGlfcmVhZCIsInVzZXJfYXBpX3dyaXRlIl0sImdlbmVyYWwtcHVycG9zZSI6dHJ1ZSwic2FtbCI6e319.E0Ca8pSTHz3sbAnDD2-oBfk5D7091Z27gq0X36Z8pfO7pIF4nD8nkk279zj3J3pbZ0qGqmSnObbZ6c-QvTADMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashup\\AppData\\Local\\Temp\\ipykernel_16828\\3569983149.py:68: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df['clean_tags']=df['tags'].str.replace('\"', '').str.replace('[', '').str.replace(']', '')\n",
      "C:\\Users\\ashup\\AppData\\Local\\Temp\\ipykernel_16828\\3569983149.py:68: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['clean_tags']=df['tags'].str.replace('\"', '').str.replace('[', '').str.replace(']', '')\n",
      "C:\\Users\\ashup\\AppData\\Local\\Temp\\ipykernel_16828\\3569983149.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['clean_tags']=df['clean_tags'].apply(lambda x: x.split(\",\"))\n",
      "C:\\Users\\ashup\\AppData\\Local\\Temp\\ipykernel_16828\\3569983149.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['clean_tags_str']  = df['clean_tags'].apply(lambda x:\" \".join(x))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = get_cleaned_dw_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "# Generate synthetic dataset with 8 random clusters\n",
    "#X, y = make_blobs(n_samples=1000, n_features=12, centers=8, random_state=42)\n",
    "\n",
    "# Instantiate the clustering model and visualizer\n",
    "model = KMeans()\n",
    "\n",
    "\n",
    "model = KModes(init = \"random\", n_init = 10, verbose=0)\n",
    "visualizer = KElbowVisualizer(model, k=(4,40),timings=False, locate_elbow=True)\n",
    "\n",
    "#visualizer.fit(X)        # Fit the data to the visualizer\n",
    "#visualizer.show()        # Finalize and render the figure\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#visualizer = KElbowVisualizer(\n",
    "#    model, k=(4,12), metric='calinski_harabasz', timings=False, locate_elbow=False\n",
    "#)\n",
    "\n",
    "visualizer.fit(X)        # Fit the data to the visualizer\n",
    "visualizer.elbow_value_       # Finalize and render the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Calinski Harabasz Score Elbow for KMeans Clustering'}, xlabel='k', ylabel='calinski harabasz score'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_ax',\n",
       " '_check_feature_names',\n",
       " '_check_n_features',\n",
       " '_fig',\n",
       " '_get_param_names',\n",
       " '_get_tags',\n",
       " '_more_tags',\n",
       " '_repr_html_',\n",
       " '_repr_html_inner',\n",
       " '_repr_mimebundle_',\n",
       " '_size',\n",
       " '_validate_data',\n",
       " '_wrapped',\n",
       " 'ax',\n",
       " 'color',\n",
       " 'colors',\n",
       " 'draw',\n",
       " 'elbow_value_',\n",
       " 'estimator',\n",
       " 'fig',\n",
       " 'finalize',\n",
       " 'fit',\n",
       " 'force_model',\n",
       " 'get_params',\n",
       " 'is_fitted',\n",
       " 'k_scores_',\n",
       " 'k_timers_',\n",
       " 'k_values_',\n",
       " 'knee_value',\n",
       " 'kneedle',\n",
       " 'locate_elbow',\n",
       " 'metric',\n",
       " 'metric_color',\n",
       " 'name',\n",
       " 'poof',\n",
       " 'score',\n",
       " 'scoring_metric',\n",
       " 'set_params',\n",
       " 'set_title',\n",
       " 'show',\n",
       " 'size',\n",
       " 'timing_color',\n",
       " 'timings',\n",
       " 'title',\n",
       " 'vline_color']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(visualizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ashup\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 4 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "\n",
      "WARNING: You are on Windows. If you detect any issue with pandarallel, be sure you checked out the Troubleshooting page:\n",
      "https://nalepae.github.io/pandarallel/troubleshooting/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ashup\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from Import_lib import *\n",
    "from Get_post_data import *\n",
    "from Post_Token_identifications import *\n",
    "#import post_scoring_script as pss\n",
    "\n",
    "import sentiment_analysis as sa\n",
    "import generate_token_level_sentiment as gsa\n",
    "import get_data_frames as gd\n",
    "import calculate_pop_hot as pop_hot\n",
    "import create_post_connections as post_con\n",
    "\n",
    "subreddit_list = ['CryptoCurrency','CryptoMarkets']\n",
    "duration = 31\n",
    "post_limit =200\n",
    "start = (2022, 2, 15)\n",
    "end = (2022,2,18)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from Import_lib import *\n",
    "from Get_post_data import *\n",
    "from Post_Token_identifications import *\n",
    "#import post_scoring_script as pss\n",
    "\n",
    "import sentiment_analysis as sa\n",
    "import generate_token_level_sentiment as gsa\n",
    "import get_data_frames as gd\n",
    "import calculate_pop_hot as pop_hot\n",
    "import create_post_connections as post_con\n",
    "\n",
    "subreddit_list = ['CryptoCurrency','CryptoMarkets']\n",
    "duration = 31\n",
    "post_limit =200\n",
    "start = (2022, 2, 15)\n",
    "end = (2022,2,18)\n",
    "\n",
    "\n",
    "print('-> scrapping reddit_data, start:',start,'end : ',end)\n",
    "\n",
    "####-------------------- 1.\n",
    "#download reddit posts \n",
    "##### writing data at : \"./redditdata/\"+subreddit_+start_epoch_raw.strftime(\"%d_%m_%y\")+\".txt\", sep=\"|\",index = False)\n",
    "get_post_data_and_summary(subreddit_list = subreddit_list,\n",
    "                          duration = duration,\n",
    "                          post_limit =post_limit, \n",
    "                          start = start, end = end)\n",
    "\n",
    "\n",
    "print('-> scrapping completed')\n",
    "print('-> identifying tokens')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####-------------------- 2.\n",
    "############\n",
    "#identify token per day and score per day\n",
    "# creating data at './reddit_post_summary_month'/trend_data_'+month+'.csv')\n",
    "post_frame = gd.get_post_data()\n",
    "generate_Data(post_frame=post_frame)\n",
    "\n",
    "\n",
    "\n",
    "print('-> token identification complete')\n",
    "print('-> estimating post Sentiments')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####-------------------- 3.\n",
    "#generate _ sentiments for the posts\n",
    "######## needs : ./redditdata/*.txt\n",
    "######## writing data at : \"../redditdata_sentiment/\"+j['date__'].iloc[0]+\".txt\", sep=\"|\",index = False)\n",
    "#[sa.get_emo_df(name,group) for name,group in post_frame.groupby('date__')]\n",
    "sa.run_onlist_loop(post_frame)\n",
    "\n",
    "\n",
    "del post_frame\n",
    "\n",
    "print('-> Sentiments identified')\n",
    "print('-> associating sentiments with tokens')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####-------------------- 4.\n",
    "#get_sentiment_post_frame\n",
    "token_frame = gd.get_post_summary_trend_data(path = './reddit_post_summary_month/trend_data_')\n",
    "sentiment_frame = gd.get_sentiment_post_frame(path = './redditdata_sentiment/')\n",
    "\n",
    "###############\n",
    "############### \n",
    "##writes ./reddit_post_summary_day/trend_data_'+name+'.csv') \n",
    "# ----- dataframe with base metric and sentiments\n",
    "gsa.run_sentiment_agg(token_frame,sentiment_frame)\n",
    "\n",
    "\n",
    "del token_frame\n",
    "del sentiment_frame\n",
    "\n",
    "\n",
    "print('-> sentiments and tokens assocaition complete')\n",
    "print('-> Calculating rolling popularity and hotness for requisite factors')\n",
    "\n",
    "####-------------------- 5.\n",
    "################\n",
    "###############\n",
    "\n",
    "token_sentiment_frame = gd.get_post_summary_trend_data(path = './reddit_post_summary_day/trend_data_')\n",
    "\n",
    "column_list = ['daily_popularity', 'roberta_Negative_sum',\n",
    "       'roberta_Negative_wt_sum', 'roberta_Negative_mean',\n",
    "       'roberta_Positive_sum', 'roberta_Positive_wt_sum',\n",
    "       'roberta_Positive_mean', 'vader_neg_sum', 'vader_neg_wt_sum',\n",
    "       'vader_neg_mean',\n",
    "       'vader_pos_sum', 'vader_pos_wt_sum', 'vader_pos_mean',\n",
    "       'distil_sadness_sum', 'distil_sadness_wt_sum', 'distil_sadness_mean',\n",
    "       'distil_joy_sum', 'distil_joy_wt_sum', 'distil_joy_mean',\n",
    "       'distil_love_sum', 'distil_love_wt_sum', 'distil_love_mean',\n",
    "       'distil_anger_sum', 'distil_anger_wt_sum', 'distil_anger_mean',\n",
    "       'distil_fear_sum', 'distil_fear_wt_sum', 'distil_fear_mean',\n",
    "       'distil_surprise_sum', 'distil_surprise_wt_sum',\n",
    "       'distil_surprise_mean']\n",
    "\n",
    "##### needs : './reddit_post_summary_day/trend_data_'):\n",
    "##### creating data at : \"./reddit_post_score_summary_month/all_summary_\"+month+\".csv\")\n",
    "\n",
    "pop_hot.calculate_popularity_hotness_rising_(token_sentiment_frame,column_list =column_list)\n",
    "\n",
    "\n",
    "del token_sentiment_frame\n",
    "\n",
    "print('-> Calculation complete')\n",
    "print('-> Creating connection nodes and edges and calculating connection strength')\n",
    "\n",
    "\n",
    "###-----------------------6.\n",
    "###########\n",
    "#processing on data\n",
    "# creates    ./reddit_topic_connections_day/ \n",
    "# creates    ./reddit_topic_weights_connections_day/\n",
    "\n",
    "token_senti_summary_frame =  gd.get_frame_for_connections(path = './reddit_post_score_summary_month/all_summary_', n=300)\n",
    "\n",
    "post_con.get_connections(token_senti_summary_frame)\n",
    "\n",
    "\n",
    "\n",
    "del token_senti_summary_frame\n",
    "\n",
    "\n",
    "\n",
    "print('-> Connections created')\n",
    "print('#### Social listening chatter data Processing Completed #####')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Calculation complete\n",
      "-> Creating connection nodes and edges and calculating connection strength\n",
      "-> Connections created\n",
      "#### Social listening chatter data Processing Completed #####\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#get_frame(path = './reddit_post_summary_day/trend_data_')\n",
    "\n",
    "\n",
    "\n",
    "#frame= gd.get_post_summary_trend_data()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#get_frame(path = './reddit_post_summary_month/trend_data_')\n",
    "\n",
    "#post_frame_2 = gd.get_sentiment_post_frame().set_index('id')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#confusion\n",
    "#frame = gd.get_post_summary_trend_data()\n",
    "#token_ata = ti.create_summary_each_day(post_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 4 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ashup\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: You are on Windows. If you detect any issue with pandarallel, be sure you checked out the Troubleshooting page:\n",
      "https://nalepae.github.io/pandarallel/troubleshooting/\n",
      "-> scrapping reddit_data, start: (2022, 2, 15) end :  (2022, 2, 18)\n",
      "-> scrapping completed\n",
      "-> identifying tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ashup\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\OMSCS\\DVA\\Final_Project - Copy\\Final Folder----\\Main_script.py\", line 47, in <module>\n",
      "    generate_Data(post_frame=post_frame)\n",
      "  File \"D:\\OMSCS\\DVA\\Final_Project - Copy\\Final Folder----\\Post_Token_identifications.py\", line 103, in generate_Data\n",
      "    if post_frame == None:\n",
      "  File \"C:\\Users\\ashup\\.conda\\envs\\new_cryptic\\lib\\site-packages\\pandas\\core\\generic.py\", line 1527, in __nonzero__\n",
      "    raise ValueError(\n",
      "ValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n"
     ]
    }
   ],
   "source": [
    "!python Main_script.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-new_cryptic]",
   "language": "python",
   "name": "conda-env-.conda-new_cryptic-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
